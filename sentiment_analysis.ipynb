{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "tweets_df = pd.read_csv('tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>geo.place_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>The Russell Wilson trade may go down as one of...</td>\n",
       "      <td>['1586724415185854467']</td>\n",
       "      <td>1586724415185854467</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Russell Wilson did all that shit on that plane...</td>\n",
       "      <td>['1586724412430098434']</td>\n",
       "      <td>1586724412430098434</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>@CourtsSports0 Russell Wilson is the answer \\n...</td>\n",
       "      <td>['1586724410135830529']</td>\n",
       "      <td>1586724410135830529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>Russell Wilson threw a pick so Nathaniel Hacke...</td>\n",
       "      <td>['1586724372365967360']</td>\n",
       "      <td>1586724372365967360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>As weird as it sounds (or would have sounded a...</td>\n",
       "      <td>['1586724357132279809']</td>\n",
       "      <td>1586724357132279809</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               text  \\\n",
       "0   en  The Russell Wilson trade may go down as one of...   \n",
       "1   en  Russell Wilson did all that shit on that plane...   \n",
       "2   en  @CourtsSports0 Russell Wilson is the answer \\n...   \n",
       "3   en  Russell Wilson threw a pick so Nathaniel Hacke...   \n",
       "4   en  As weird as it sounds (or would have sounded a...   \n",
       "\n",
       "    edit_history_tweet_ids                   id geo.place_id  \n",
       "0  ['1586724415185854467']  1586724415185854467          NaN  \n",
       "1  ['1586724412430098434']  1586724412430098434          NaN  \n",
       "2  ['1586724410135830529']  1586724410135830529          NaN  \n",
       "3  ['1586724372365967360']  1586724372365967360          NaN  \n",
       "4  ['1586724357132279809']  1586724357132279809          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = [\"The\", \"It\", \"it\", \"in\", \"In\", \"wh\"]\n",
    "\n",
    "tweets_df['text'] = [re.sub(\"https?:\\/\\/\\S+\", \"\", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"#[A-Za-z0–9]+\", \" \", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"#\", \" \", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"\\n\", \" \", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"@[A-Za-z0–9]+\", \"\", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"RT\", \"\", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"^[a-zA-Z]{1,2}$\", \"\", tweet) for tweet in tweets_df['text']]\n",
    "tweets_df['text'] = [re.sub(\"\\w*\\d\\w*\", \"\", tweet) for tweet in tweets_df['text']]\n",
    "for word in extra_stopwords:\n",
    "    tweets_df['text'] = [tweet.replace(word, \"\") for tweet in tweets_df['text']]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sali/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sali/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text_tokens = w_tokenizer.tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(w) for w in text_tokens if not w in stopwords.words('english'))\n",
    "    # return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "tweets_df['clean_text'] = tweets_df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def getPolarity(tweet):\n",
    "    sentiment_polarity = TextBlob(tweet).sentiment.polarity\n",
    "    return sentiment_polarity\n",
    "\n",
    "def getAnalysis(polarity_score):\n",
    "    if polarity_score < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity_score == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "def getSubjectivity(tweet):\n",
    "    sentiment_subjectivity = TextBlob(tweet).sentiment.subjectivity\n",
    "    return sentiment_subjectivity\n",
    "\n",
    "def getSubAnalysis(subjectivity_score):\n",
    "    if subjectivity_score <= 0.5:\n",
    "        return \"Objective\"\n",
    "    else:\n",
    "        return \"Subjective\"\n",
    "        \n",
    "tweets_df[\"polarity\"] = tweets_df[\"clean_text\"].apply(getPolarity)\n",
    "tweets_df[\"sentiment\"] = tweets_df[\"polarity\"].apply(getAnalysis)\n",
    "tweets_df[\"subjectivity\"] = tweets_df[\"clean_text\"].apply(getSubjectivity)\n",
    "tweets_df[\"sub_obj\"] = tweets_df[\"subjectivity\"].apply(getSubAnalysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('minimal_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cb0ae753b5d661b55b24c9697833d78e18d2d2409669606bbc4f773b5a3fac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
